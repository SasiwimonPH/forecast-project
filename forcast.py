# -*- coding: utf-8 -*-
"""forcast.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JCbaQMvAZTJDrZPgRN0DJz7WCTnrXSto
"""

# Install specific versions of packages to ensure compatibility
!pip install scipy==1.11.4 statsmodels==0.14.0 pmdarima==2.0.4 --upgrade --force-reinstall

import pandas as pd

# ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
df = pd.read_csv("sales2022_2023.csv")

df

df['Branch_id'] = df['BranchName'].str.split('-').str[0]
df = df.drop(columns=['BranchName'])
df = df[~df['Branch_id'].isin(['EK15', 'EK17','EK18','EK19','EK20'])]

df

import matplotlib.pyplot as plt
import pandas as pd

# ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡πÉ‡∏´‡∏°‡πà‡∏ä‡∏∑‡πà‡∏≠ YearMonth ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏Å‡∏ô X (‡∏õ‡∏µ‡πÅ‡∏•‡∏∞‡πÄ‡∏î‡∏∑‡∏≠‡∏ô‡∏£‡∏ß‡∏°‡∏Å‡∏±‡∏ô)
df['Date'] = pd.to_datetime(df[['Year', 'Month']].assign(DAY=1))
# ‡πÄ‡∏ï‡∏¥‡∏° missing values
for col in ['SalesAmount', 'Bill', 'Member', 'Sales Per Visit']:
    df[col] = df[col].fillna(df[col].median())

branches = df['Branch_id'].unique()

plt.figure(figsize=(15,8))

for branch in branches:
    df_branch = df[df['Branch_id'] == branch]
    plt.plot(df_branch['Date'], df_branch['SalesAmount'], marker='o', label=branch)

    # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏õ‡πâ‡∏≤‡∏¢‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡∏¢‡∏≠‡∏î‡∏Ç‡∏≤‡∏¢
    for x, y in zip(df_branch['Date'], df_branch['SalesAmount']):
        plt.text(x, y, f'{y:,}', fontsize=9, ha='center', va='bottom', rotation=45)

plt.title('Monthly Sales Amount by Branch')
plt.xlabel('Date')
plt.ylabel('Sales Amount')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

"""‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå‡∏¢‡∏≠‡∏î‡∏Ç‡∏≤‡∏¢‡∏Å‡∏±‡∏ö‡∏õ‡∏±‡∏à‡∏à‡∏±‡∏¢"""

import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd

# ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå
features = ['SalesAmount', 'Bill', 'Member', 'Sales Per Visit',
            'Promotion_disc(%)', 'Promotion_BuygetFree', 'Promotion_gift',
            'Promotion_member', 'Promotion_platfrom', 'Promotion_set']

# ‡πÄ‡∏ï‡∏¥‡∏° missing values
for col in features:
    if col in df.columns:
      df[col] = df.groupby('Branch_id')[col].transform(lambda x: x.fillna(x.median()))


branches = df['Branch_id'].unique()

for branch in branches:
    df_branch = df[df['Branch_id'] == branch].copy()
    branch_features = [f for f in features if f in df_branch.columns]
    if not branch_features:
        print(f"No relevant features found for correlation in branch: {branch}")
        continue

    corr_matrix = df_branch[branch_features].corr(method='pearson')

    plt.figure(figsize=(10, 8))
    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=".2f", linewidths=0.5)
    plt.title(f'Correlation Heatmap - {branch}')
    plt.tight_layout()
    plt.show()

# ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå‡∏ß‡∏±‡∏ô‡∏´‡∏¢‡∏∏‡∏î
holiday_df = pd.read_csv("thai_monthly_holiday_features.csv")

# ‡∏£‡∏ß‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏î‡πâ‡∏ß‡∏¢ merge ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ year-month
merged_df = df.merge(holiday_df, how='left', left_on=['Year', 'Month'], right_on=['year', 'month'])

#‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå year ‡∏Å‡∏±‡∏ö month ‡∏ã‡πâ‡∏≥‡∏≠‡∏≠‡∏Å
merged_df = merged_df.drop(columns=['year', 'month','major_holiday'])

merged_df

merged_df = merged_df.fillna(0)
print(merged_df.isnull().sum())

merged_df = merged_df.drop(columns=['Promotion'])

merged_df

savedata= merged_df.to_csv('merged_df.csv', index=False)

# ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 1: Import ‡πÅ‡∏•‡∏∞‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
import pandas as pd
import numpy as np
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error
from statsmodels.tsa.statespace.sarimax import SARIMAX
from statsmodels.tsa.stattools import adfuller
import pmdarima as pm

# ‡∏™‡∏°‡∏°‡∏∏‡∏ï‡∏¥‡∏ß‡πà‡∏≤ merged_df ‡∏Ñ‡∏∑‡∏≠ DataFrame ‡∏´‡∏•‡∏±‡∏Å‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì
df = merged_df.copy()
df['Date'] = pd.to_datetime(df[['Year', 'Month']].assign(DAY=1))

if 'BranchName' not in df.columns:
    df['BranchName'] = df['Branch_id']

df = df.sort_values(['BranchName', 'Date'])

# ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå
features = ['Bill','Member','Sales Per Visit','Promotion_disc(%)',
            'Promotion_BuygetFree','Promotion_gift','Promotion_member',
            'Promotion_platfrom','Promotion_set','num_holidays','has_long_weekend']
target = 'SalesAmount'
correlation_threshold = 0.3

# ‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡πÄ‡∏Å‡πá‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÅ‡∏•‡∏∞‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
mlr_models = {}
arimax_models = {}
results = []
combined_predictions = []

# ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 2: ‡∏ß‡∏ô‡∏•‡∏π‡∏õ‡∏ù‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏™‡∏≤‡∏Ç‡∏≤
for branch, group in df.groupby('BranchName'):
    group = group.sort_values('Date').reset_index(drop=True)
    split_idx = int(len(group) * 0.8)
    train = group.iloc[:split_idx].reset_index(drop=True)
    test = group.iloc[split_idx:].reset_index(drop=True)

    if len(train) < 10 or len(test) < 1:
        print(f"[{branch}] Not enough data.")
        continue

    available_features = [f for f in features if f in train.columns]
    corr = train[available_features + [target]].corr(method='pearson')[target].drop(target)
    selected_features = corr[abs(corr) >= correlation_threshold].index.tolist()

    if not selected_features:
        print(f"[{branch}] No features selected.")
        continue

    X_train = train[selected_features].fillna(0)
    y_train = train[target].fillna(method='ffill')
    X_test = test[selected_features].fillna(0)
    y_test = test[target].fillna(method='ffill')

    # -------- MLR --------
    try:
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.transform(X_test)

        mlr = LinearRegression()
        mlr.fit(X_train_scaled, y_train)

        y_pred_train_mlr = mlr.predict(X_train_scaled)
        y_pred_test_mlr = mlr.predict(X_test_scaled)

        mlr_models[branch] = {
            'model': mlr,
            'scaler': scaler,
            'features': selected_features
        }

        # Evaluation: MLR
        mlr_metrics = {
            'rmse_train': np.sqrt(mean_squared_error(y_train, y_pred_train_mlr)),
            'mape_train': mean_absolute_percentage_error(y_train, y_pred_train_mlr) * 100,
            'mae_train': mean_absolute_error(y_train, y_pred_train_mlr),
            'rmse_test': np.sqrt(mean_squared_error(y_test, y_pred_test_mlr)),
            'mape_test': mean_absolute_percentage_error(y_test, y_pred_test_mlr) * 100,
            'mae_test': mean_absolute_error(y_test, y_pred_test_mlr)
        }

        results.append({
            'Branch': branch,
            'Model': 'MLR',
            'Train_RMSE': mlr_metrics['rmse_train'],
            'Train_MAPE': mlr_metrics['mape_train'],
            'Train_MAE': mlr_metrics['mae_train'],
            'Test_RMSE': mlr_metrics['rmse_test'],
            'Test_MAPE': mlr_metrics['mape_test'],
            'Test_MAE': mlr_metrics['mae_test'],
            'Used_Features': ', '.join(selected_features)
        })

    except Exception as e:
        print(f"[{branch}] MLR error: {e}")
        continue

    # -------- ARIMAX --------
    try:
        diff_order = 0
        if len(y_train.dropna()) > 0:
            pval = adfuller(y_train.dropna())[1]
            diff_order = 0 if pval < 0.05 else 1

        auto_model = pm.auto_arima(y_train, exogenous=X_train, d=diff_order,
                                   seasonal=False, stepwise=True,
                                   error_action='ignore', suppress_warnings=True)
        best_order = auto_model.order

        arimax_model = SARIMAX(endog=y_train, exog=X_train, order=best_order,
                               enforce_stationarity=False, enforce_invertibility=False)
        arimax_result = arimax_model.fit(disp=False)

        y_pred_train_arimax = arimax_result.predict(start=0, end=len(train)-1, exog=X_train)
        y_pred_test_arimax = arimax_result.predict(start=len(train), end=len(train)+len(test)-1, exog=X_test)

        arimax_models[branch] = {
            'model': arimax_result,
            'features': selected_features,
            'order': best_order
        }

        # Evaluation: ARIMAX
        arimax_metrics = {
            'rmse_train': np.sqrt(mean_squared_error(y_train, y_pred_train_arimax)),
            'mape_train': mean_absolute_percentage_error(y_train, y_pred_train_arimax) * 100,
            'mae_train': mean_absolute_error(y_train, y_pred_train_arimax),
            'rmse_test': np.sqrt(mean_squared_error(y_test, y_pred_test_arimax)),
            'mape_test': mean_absolute_percentage_error(y_test, y_pred_test_arimax) * 100,
            'mae_test': mean_absolute_error(y_test, y_pred_test_arimax)
        }

        results.append({
            'Branch': branch,
            'Model': f'ARIMAX {best_order}',
            'Train_RMSE': arimax_metrics['rmse_train'],
            'Train_MAPE': arimax_metrics['mape_train'],
            'Train_MAE': arimax_metrics['mae_train'],
            'Test_RMSE': arimax_metrics['rmse_test'],
            'Test_MAPE': arimax_metrics['mape_test'],
            'Test_MAE': arimax_metrics['mae_test'],
            'Used_Features': ', '.join(selected_features)
        })

    except Exception as e:
        print(f"[{branch}] ARIMAX error: {e}")
        continue

    # -------- ‡∏£‡∏ß‡∏°‡∏Ñ‡πà‡∏≤‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏ì‡πå --------
    combined_df = pd.DataFrame({
        'Branch': branch,
        'Date': pd.concat([train['Date'], test['Date']]).values,
        'y_true': pd.concat([y_train, y_test]).values,
        'y_pred_mlr': np.concatenate([y_pred_train_mlr, y_pred_test_mlr]),
        'y_pred_arimax': np.concatenate([y_pred_train_arimax, y_pred_test_arimax])
    })

    combined_predictions.append(combined_df)

# ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 3: ‡∏£‡∏ß‡∏°‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
df_all_preds = pd.concat(combined_predictions, ignore_index=True)
df_all_preds = df_all_preds.sort_values(['Branch', 'Date']).reset_index(drop=True)

print("Combined Predictions:")
df_all_preds

# ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 3: ‡∏£‡∏ß‡∏°‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
df_results = pd.DataFrame(results)
df_results = df_results.sort_values(['Branch', 'Model']).reset_index(drop=True)

# ‡πÅ‡∏™‡∏î‡∏á‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
print("Evaluation Summary:")
df_results

df_results[['Branch', 'Model', 'Test_RMSE', 'Test_MAPE', 'Test_MAE']]

"""üîπ MLR
‡∏ä‡∏ô‡∏∞‡πÉ‡∏ô ~73% ‡∏Ç‡∏≠‡∏á‡∏™‡∏≤‡∏Ç‡∏≤

‡πÉ‡∏´‡πâ‡∏ú‡∏•‡∏î‡∏µ‡∏Å‡∏ß‡πà‡∏≤ ARIMAX ‡πÉ‡∏ô ‡∏™‡∏≤‡∏Ç‡∏≤‡πÄ‡∏Å‡∏∑‡∏≠‡∏ö‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î

‡πÇ‡∏î‡∏¢‡πÄ‡∏â‡∏û‡∏≤‡∏∞ Test_MAPE ‡∏ï‡πà‡∏≥‡∏°‡∏≤‡∏Å (< 2%) ‡πÉ‡∏ô‡∏´‡∏•‡∏≤‡∏¢‡∏™‡∏≤‡∏Ç‡∏≤ ‡πÄ‡∏ä‡πà‡∏ô EK02, EK04, EK05, EK11, EK13

üî∏ ARIMAX
‡∏ä‡∏ô‡∏∞‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ö‡∏≤‡∏á‡∏™‡∏≤‡∏Ç‡∏≤ ‡πÄ‡∏ä‡πà‡∏ô:

EK06 ‚Üí RMSE, MAPE, MAE ‡∏î‡∏µ‡∏Å‡∏ß‡πà‡∏≤

EK07 ‚Üí ‡∏î‡∏µ‡∏Å‡∏ß‡πà‡∏≤‡πÉ‡∏ô‡∏ó‡∏∏‡∏Å‡∏ï‡∏±‡∏ß‡∏ä‡∏µ‡πâ‡∏ß‡∏±‡∏î

EK10 ‚Üí RMSE ‡πÅ‡∏•‡∏∞ MAE ‡∏î‡∏µ‡∏Å‡∏ß‡πà‡∏≤

EK16 ‚Üí ‡∏î‡∏µ‡∏Å‡∏ß‡πà‡∏≤‡πÄ‡∏•‡πá‡∏Å‡∏ô‡πâ‡∏≠‡∏¢‡πÉ‡∏ô‡∏ó‡∏∏‡∏Å‡∏ï‡∏±‡∏ß‡∏ä‡∏µ‡πâ‡∏ß‡∏±‡∏î

‡∏°‡∏µ‡∏ö‡∏≤‡∏á‡∏Å‡∏£‡∏ì‡∏µ‡∏ó‡∏µ‡πà ARIMAX ‡πÅ‡∏¢‡πà‡∏°‡∏≤‡∏Å ‡πÄ‡∏ä‡πà‡∏ô EK05, EK11 ‚Üí ‡∏Ñ‡πà‡∏≤‡∏Ñ‡∏≤‡∏î‡πÄ‡∏Ñ‡∏•‡∏∑‡πà‡∏≠‡∏ô‡∏™‡∏π‡∏á‡∏Å‡∏ß‡πà‡∏≤‡∏´‡∏•‡∏≤‡∏¢‡πÄ‡∏ó‡πà‡∏≤
"""

from sklearn.model_selection import TimeSeriesSplit

merged_df = merged_df.sort_values(['Branch_id', 'Date']).reset_index(drop=True)

n_splits = 5
tscv = TimeSeriesSplit(n_splits=n_splits)

print(f"Performing Time Series Cross-Validation with {n_splits} splits.")

cv_splits = []
for train_index, test_index in tscv.split(merged_df):
    cv_splits.append((train_index, test_index))

print(f"Generated {len(cv_splits)} splits.")

from sklearn.linear_model import LinearRegression
from statsmodels.tsa.statespace.sarimax import SARIMAX
import pmdarima as pm
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_absolute_percentage_error, mean_squared_error, mean_absolute_error
import numpy as np
import pandas as pd

# Define the features and target
features = ['Bill','Member','Sales Per Visit','Promotion_disc(%)', 'Promotion_BuygetFree',
             'Promotion_gift', 'Promotion_member', 'Promotion_platfrom', 'Promotion_set','num_holidays', 'has_long_weekend']
target = 'SalesAmount'

# Set correlation threshold for feature selection
correlation_threshold = 0.3

cv_results = []

for branch_name, branch_data in merged_df.groupby('Branch_id'):
    print(f"Processing branch: {branch_name}")

    # Prepare data for this branch
    branch_data = branch_data.reset_index(drop=True)
    X_all = branch_data[features].fillna(0)
    y_all = branch_data[target].fillna(method='ffill')

    if len(branch_data) < n_splits + 1:
        print(f"  Skipping branch {branch_name}: Not enough data for {n_splits} splits.")
        continue

    # --- Cross-validation loop ---
    for fold_num, (train_index, test_index) in enumerate(tscv.split(branch_data), 1):
        print(f"  Processing Fold {fold_num}/{n_splits}")

        train_fold = branch_data.iloc[train_index]
        test_fold = branch_data.iloc[test_index]

        X_train = train_fold[features].fillna(0)
        y_train = train_fold[target].fillna(method='ffill')

        X_test = test_fold[features].fillna(0)
        y_test = test_fold[target].fillna(method='ffill')


        if len(train_fold) < 2 or len(test_fold) < 1:
            print(f"    Skipping Fold {fold_num}: Not enough data in train or test split.")
            continue

        # --- Feature Selection for the current fold (based on train data) ---
        available_features_fold = [f for f in features if f in X_train.columns]
        if not available_features_fold:
             print(f"    Skipping Fold {fold_num}: No available features for correlation analysis.")
             cv_results.append({
                'Branch': branch_name,
                'fold': fold_num,
                'Model': 'MLR',
                'Train_RMSE': np.nan, 'Train_MAPE': np.nan, 'Train_MAE': np.nan,
                'Test_RMSE': np.nan, 'Test_MAPE': np.nan, 'Test_MAE': np.nan,
                'Used_Features': '',
                'ARIMAX_Order': None,
                'ARIMAX_Diff_Order': np.nan
            })
             cv_results.append({
                'Branch': branch_name,
                'fold': fold_num,
                'Model': 'ARIMAX',
                'Train_RMSE': np.nan, 'Train_MAPE': np.nan, 'Train_MAE': np.nan,
                'Test_RMSE': np.nan, 'Test_MAPE': np.nan, 'Test_MAE': np.nan,
                'Used_Features': '',
                'ARIMAX_Order': None,
                'ARIMAX_Diff_Order': np.nan
            })
             continue

        corr_fold = X_train[available_features_fold].corrwith(y_train)
        selected_features_fold = corr_fold[abs(corr_fold) >= correlation_threshold].index.tolist()

        if len(selected_features_fold) == 0:
            print(f"    Skipping Fold {fold_num}: No features selected based on correlation threshold.")

            cv_results.append({
                'Branch': branch_name,
                'fold': fold_num,
                'Model': 'MLR',
                'Train_RMSE': np.nan, 'Train_MAPE': np.nan, 'Train_MAE': np.nan,
                'Test_RMSE': np.nan, 'Test_MAPE': np.nan, 'Test_MAE': np.nan,
                'Used_Features': '',
                'ARIMAX_Order': None,
                'ARIMAX_Diff_Order': np.nan
            })
            cv_results.append({
                'Branch': branch_name,
                'fold': fold_num,
                'Model': 'ARIMAX',
                'Train_RMSE': np.nan, 'Train_MAPE': np.nan, 'Train_MAE': np.nan,
                'Test_RMSE': np.nan, 'Test_MAPE': np.nan, 'Test_MAE': np.nan,
                'Used_Features': '',
                'ARIMAX_Order': None,
                'ARIMAX_Diff_Order': np.nan
            })
            continue

        X_train_selected = X_train[selected_features_fold]
        X_test_selected = X_test[selected_features_fold]

        # --- Multiple Linear Regression for the fold ---
        try:
            scaler = StandardScaler()
            X_train_scaled = scaler.fit_transform(X_train_selected)
            X_test_scaled = scaler.transform(X_test_selected)

            mlr = LinearRegression()
            mlr.fit(X_train_scaled, y_train)
            y_pred_train_mlr = mlr.predict(X_train_scaled)
            y_pred_test_mlr = mlr.predict(X_test_scaled)

            rmse_train_mlr = np.sqrt(mean_squared_error(y_train, y_pred_train_mlr))
            mape_train_mlr = mean_absolute_percentage_error(y_train, y_pred_train_mlr) * 100
            mae_train_mlr = mean_absolute_error(y_train, y_pred_train_mlr)

            rmse_test_mlr = np.sqrt(mean_squared_error(y_test, y_pred_test_mlr))
            mape_test_mlr = mean_absolute_percentage_error(y_test, y_pred_test_mlr) * 100
            mae_test_mlr = mean_absolute_error(y_test, y_pred_test_mlr)
            cv_results.append({
                'Branch': branch_name,
                'fold': fold_num,
                'Model': 'MLR',
                'Train_RMSE': rmse_train_mlr,
                'Train_MAPE': mape_train_mlr,
                'Train_MAE': mae_train_mlr,
                'Test_RMSE': rmse_test_mlr,
                'Test_MAPE': mape_test_mlr,
                'Test_MAE': mae_test_mlr,
                'Used_Features': ', '.join(selected_features_fold),
                'ARIMAX_Order': None,
                'ARIMAX_Diff_Order': np.nan
            })

        except Exception as e:
            print(f"    MLR Error in Fold {fold_num} for branch {branch_name}: {e}")
            cv_results.append({
                'Branch': branch_name,
                'fold': fold_num,
                'Model': 'MLR',
                'Train_RMSE': np.nan, 'Train_MAPE': np.nan, 'Train_MAE': np.nan,
                'Test_RMSE': np.nan, 'Test_MAPE': np.nan, 'Test_MAE': np.nan,
                'Used_Features': ', '.join(selected_features_fold),
                'ARIMAX_Order': None,
                'ARIMAX_Diff_Order': np.nan
            })


        # --- ARIMAX with auto_arima for the fold ---
        try:
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Stationarity ‡∏î‡πâ‡∏ß‡∏¢ ADF Test
            adf_result = adfuller(y_train.dropna())
            is_stationary = adf_result[1] < 0.05
            diff_order = 0 if is_stationary else 1

            # ‡πÉ‡∏ä‡πâ auto_arima ‡∏´‡∏≤ best order
            auto_model = pm.auto_arima(
                y_train,
                exogenous=X_train_selected,
                seasonal=False,
                stepwise=True,
                d=diff_order,
                error_action='ignore',
                suppress_warnings=True,
                max_p=5, max_q=5,
                trace=False
            )
            best_order = auto_model.order

            # Fit SARIMAX
            arimax_model = SARIMAX(
                endog=y_train,
                exog=X_train_selected,
                order=best_order,
                enforce_stationarity=False,
                enforce_invertibility=False
            )
            arimax_result = arimax_model.fit(disp=False)

            # ‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏ì‡πå
            y_pred_train_arimax = arimax_result.predict(start=0, end=len(train_fold)-1, exog=X_train_selected)
            y_pred_test_arimax = arimax_result.predict(start=len(train_fold), end=len(train_fold)+len(test_fold)-1, exog=X_test_selected)

            rmse_train_arimax = np.sqrt(mean_squared_error(y_train, y_pred_train_arimax))
            mape_train_arimax = mean_absolute_percentage_error(y_train, y_pred_train_arimax) * 100
            mae_train_arimax = mean_absolute_error(y_train, y_pred_train_arimax)

            rmse_test_arimax = np.sqrt(mean_squared_error(y_test, y_pred_test_arimax))
            mape_test_arimax = mean_absolute_percentage_error(y_test, y_pred_test_arimax) * 100
            mae_test_arimax = mean_absolute_error(y_test, y_pred_test_arimax)


            cv_results.append({
                'Branch': branch_name,
                'fold': fold_num,
                'Model': f'ARIMAX {best_order}',
                'Train_RMSE': rmse_train_arimax,
                'Train_MAPE': mape_train_arimax,
                'Train_MAE': mae_train_arimax,
                'Test_RMSE': rmse_test_arimax,
                'Test_MAPE': mape_test_arimax,
                'Test_MAE': mae_test_arimax,
                'Used_Features': ', '.join(selected_features_fold),
                'ARIMAX_Order': best_order,
                'ARIMAX_Diff_Order': diff_order
            })

        except Exception as e:
            print(f"    ARIMAX Error in Fold {fold_num} for branch {branch_name}: {e}")
            cv_results.append({
                'Branch': branch_name,
                'fold': fold_num,
                'Model': 'ARIMAX',
                'Train_RMSE': np.nan, 'Train_MAPE': np.nan, 'Train_MAE': np.nan,
                'Test_RMSE': np.nan, 'Test_MAPE': np.nan, 'Test_MAE': np.nan,
                'Used_Features': ', '.join(selected_features_fold),
                'ARIMAX_Order': None,
                'ARIMAX_Diff_Order': np.nan
            })

cv_df = pd.DataFrame(cv_results)

cv_summary = cv_df.groupby(['Branch', 'Model']).agg({
    'Test_RMSE': 'mean',
    'Test_MAPE': 'mean',
    'Test_MAE': 'mean',
}).reset_index()

cv_summary

"""‡∏à‡∏≤‡∏Å‡∏Ñ‡πà‡∏≤ Cross Validation, ARIMAX (‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡πÉ‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏™‡∏≤‡∏Ç‡∏≤) ‡∏°‡∏µ ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢‡∏î‡∏µ‡∏Å‡∏ß‡πà‡∏≤ MLR ‡∏ó‡∏∏‡∏Å‡∏ï‡∏±‡∏ß‡∏ä‡∏µ‡πâ‡∏ß‡∏±‡∏î

‡πÇ‡∏î‡∏¢‡πÄ‡∏â‡∏û‡∏≤‡∏∞ RMSE ‡πÅ‡∏•‡∏∞ MAE ‡∏ó‡∏µ‡πà‡∏ï‡πà‡∏≥‡∏Å‡∏ß‡πà‡∏≤‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô ‡πÅ‡∏™‡∏î‡∏á‡∏ß‡πà‡∏≤ ARIMAX ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏Å‡∏±‡∏ö‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ô‡∏µ‡πâ‡πÉ‡∏ô‡πÄ‡∏ä‡∏¥‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤

‡πÅ‡∏ï‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏£‡∏∞‡∏ß‡∏±‡∏á‡∏ß‡πà‡∏≤ ‡∏ú‡∏•‡∏Ç‡∏≠‡∏á ARIMAX ‡πÅ‡∏õ‡∏£‡∏ú‡∏±‡∏ô‡∏ï‡∏≤‡∏° parameter (p,d,q) ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏°‡∏≤‡∏Å ‡πÅ‡∏•‡∏∞‡∏≠‡∏≤‡∏à‡∏°‡∏µ overfitting ‡∏´‡∏£‡∏∑‡∏≠ instability ‡πÑ‡∏î‡πâ ‡∏´‡∏≤‡∏Å‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÑ‡∏°‡πà‡∏î‡∏µ
"""

best_indices = cv_summary.loc[cv_summary.groupby('Branch')['Test_RMSE'].idxmin()].index

# Select the best models and include the 'Used_Features' column
best_models = cv_summary.loc[best_indices, ['Branch', 'Model', 'Test_RMSE', 'Test_MAPE', 'Test_MAE']].reset_index(drop=True)

# ‡πÉ‡∏ä‡πâ best_indices ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏∂‡∏á‡πÅ‡∏ñ‡∏ß‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏á‡∏à‡∏≤‡∏Å cv_df (‡∏ã‡∏∂‡πà‡∏á‡∏°‡∏µ Used_Features ‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß)
best_cv_rows = cv_df.loc[cv_df.groupby('Branch')['Test_RMSE'].idxmin()].reset_index(drop=True)

# ‡∏£‡∏ß‡∏°‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå Used_Features ‡πÄ‡∏Ç‡πâ‡∏≤ best_models
best_models = cv_summary.loc[best_indices].reset_index(drop=True)
best_models['Used_Features'] = best_cv_rows['Used_Features'].values
best_models

"""‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ñ‡πà‡∏≤‡∏à‡∏£‡∏¥‡∏á ‡∏Ñ‡πà‡∏≤‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏ì‡πå‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏ü‡∏•‡πå csv"""

df_future = pd.read_csv("2024_pro.csv")
df_future

df_future['Branch_id'] = df_future['BranchName'].str.split('-').str[0]
df_future = df_future.drop(columns=['BranchName'])

df_future['Date'] = pd.to_datetime(df_future[['Year', 'Month']].assign(DAY=1))

# ‡∏£‡∏ß‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏î‡πâ‡∏ß‡∏¢ merge ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ year-month
df_data= df_future.merge(holiday_df, how='left', left_on=['Year', 'Month'], right_on=['year', 'month'])

#‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå year ‡∏Å‡∏±‡∏ö month ‡∏ã‡πâ‡∏≥‡∏≠‡∏≠‡∏Å
df_data = df_data.drop(columns=['year', 'month','major_holiday'])

df_data

for holidays in ['num_holidays', 'has_long_weekend']:
    df_data[holidays] = df_data[holidays].fillna(0).astype(int)

# 1. ‡∏´‡∏≤‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢‡πÉ‡∏ô merged_df ‡∏ï‡∏≤‡∏°‡∏™‡∏≤‡∏Ç‡∏≤
mean_per_branch = merged_df.groupby('Branch_id')[['Bill', 'Member', 'Sales Per Visit']].mean().reset_index()

# 2. ‡∏ô‡∏≥‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏°‡∏≤ merge ‡∏•‡∏á‡πÉ‡∏ô df_data ‡∏ï‡∏≤‡∏°‡∏™‡∏≤‡∏Ç‡∏≤
df_data = df_data.drop(columns=['Bill', 'Member', 'Sales Per Visit'], errors='ignore')  # ‡∏•‡∏ö‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡πÄ‡∏î‡∏¥‡∏°‡∏ñ‡πâ‡∏≤‡∏°‡∏µ
df_data = df_data.merge(mean_per_branch, on='Branch_id', how='left')

df_data

savedata= df_data.to_csv('2024_pro1.csv', index=False)

# ‡∏™‡∏°‡∏°‡∏∏‡∏ï‡∏¥ df_data ‡∏°‡∏µ‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå‡∏≠‡∏ô‡∏≤‡∏Ñ‡∏ï (3 ‡πÄ‡∏î‡∏∑‡∏≠‡∏ô‡∏ñ‡∏±‡∏î‡πÑ‡∏õ) ‡∏û‡∏£‡πâ‡∏≠‡∏°
future_predictions = []

for branch, branch_data in df_data.groupby("Branch_id"):
    if branch not in mlr_models or branch not in arimax_models:
        print(f"[{branch}] ‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡∏ù‡∏∂‡∏Å‡πÑ‡∏ß‡πâ")
        continue

    branch_future = branch_data.sort_values("Date").reset_index(drop=True)
    branch_future = branch_future.head(3)

    selected_features = mlr_models[branch]['features']
    if any(f not in branch_future.columns for f in selected_features):
        print(f"[{branch}] ‡∏Ç‡∏≤‡∏î‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô")
        continue

    X_future = branch_future[selected_features].fillna(0)

    # MLR
    scaler = mlr_models[branch]['scaler']
    X_scaled = scaler.transform(X_future)
    y_pred_mlr = mlr_models[branch]['model'].predict(X_scaled)

    # ARIMAX
    try:
        past_group = df[df['BranchName'] == branch].sort_values("Date")
        y_past = past_group[target].fillna(method='ffill')
        X_past = past_group[selected_features].fillna(0)

        arimax_model = SARIMAX(endog=y_past, exog=X_past, order=arimax_models[branch]['order'],
                               enforce_stationarity=False, enforce_invertibility=False)
        fitted = arimax_model.fit(disp=False)

        y_pred_arimax = fitted.predict(start=len(y_past), end=len(y_past)+len(X_future)-1, exog=X_future)

    except Exception as e:
        print(f"[{branch}] ARIMAX prediction error: {e}")
        y_pred_arimax = [np.nan] * len(X_future)

    # ‡∏£‡∏ß‡∏°
    pred_df = pd.DataFrame({
        'Branch': branch,
        'Date': branch_future['Date'].values,
        'y_pred_mlr': y_pred_mlr,
        'y_pred_arimax': y_pred_arimax
    })

    future_predictions.append(pred_df)

df_future_forecast = pd.concat(future_predictions, ignore_index=True)
df_future_forecast = df_future_forecast.sort_values(['Branch', 'Date']).reset_index(drop=True)

# ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•
print("‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏ì‡πå‡∏¢‡∏≠‡∏î‡∏Ç‡∏≤‡∏¢‡∏•‡πà‡∏ß‡∏á‡∏´‡∏ô‡πâ‡∏≤ 3 ‡πÄ‡∏î‡∏∑‡∏≠‡∏ô")
df_future_forecast

# ‡∏£‡∏ß‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏¢‡πâ‡∏≠‡∏ô‡∏´‡∏•‡∏±‡∏á‡πÅ‡∏•‡∏∞‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏ì‡πå
df_combined_all = pd.concat([df_all_preds_aligned, df_future_forecast], ignore_index=True)
df_combined_all = df_combined_all.sort_values(['Branch', 'Date']).reset_index(drop=True)

# ‡∏•‡∏ö‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå type
df_combined_all.drop(columns='type', inplace=True)

# ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•
df_combined_all

!pip install --quiet gspread gspread_dataframe oauth2client

import pandas as pd
import gspread
from gspread_dataframe import set_with_dataframe
from oauth2client.service_account import ServiceAccountCredentials

import os
import json
from oauth2client.service_account import ServiceAccountCredentials

scope = ["https://spreadsheets.google.com/feeds", "https://www.googleapis.com/auth/drive"]

# ‡πÇ‡∏´‡∏•‡∏î JSON ‡∏à‡∏≤‡∏Å environment variable
creds_json = os.environ["GCP_CREDENTIALS_JSON"]
creds_dict = json.loads(creds_json)

creds = ServiceAccountCredentials.from_json_keyfile_dict(creds_dict, scope)

# ‡πÄ‡∏õ‡∏¥‡∏î Google Sheets (‡πÉ‡∏ä‡πâ‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡πÅ‡∏ä‡∏£‡πå‡πÑ‡∏ß‡πâ‡∏Å‡∏±‡∏ö Service Account)
spreadsheet = client.open("Project")
sheet = spreadsheet.sheet1  # ‡∏ñ‡πâ‡∏≤‡πÉ‡∏ä‡πâ worksheet ‡πÅ‡∏£‡∏Å
# ‡∏´‡∏£‡∏∑‡∏≠‡πÉ‡∏ä‡πâ: spreadsheet.worksheet("‡∏ä‡∏∑‡πà‡∏≠‡πÅ‡∏ú‡πà‡∏ô‡∏á‡∏≤‡∏ô") ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡∏´‡∏•‡∏≤‡∏¢‡πÅ‡∏ú‡πà‡∏ô

# ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô DataFrame ‡∏•‡∏á‡πÉ‡∏ô Google Sheet (‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ó‡∏µ‡πà‡πÄ‡∏ã‡∏•‡∏•‡πå A1)
set_with_dataframe(sheet, df_combined_all)

print("‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î DataFrame ‡πÑ‡∏õ‡∏¢‡∏±‡∏á Google Sheets ‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢‡πÅ‡∏•‡πâ‡∏ß!")